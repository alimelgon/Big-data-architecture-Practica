{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Scrapy alicia hoteles modificado",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alimelgon/Big-data-architecture-Practica/blob/master/Scrapy_alicia_hoteles_modificado.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "173c-bcUjcE4",
        "colab_type": "text"
      },
      "source": [
        "## NOTA: Crea una carpeta en tu drive llamada webcrawling para que te funcione la subida a drive sin modificar nombres de ficheros/carpetas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fsnh0LjlYg3g",
        "colab_type": "text"
      },
      "source": [
        "## Instalar scrapy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7iWVmA7ZwjU",
        "colab_type": "code",
        "outputId": "b0207522-1c13-456f-f266-bb3e25faf7aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        }
      },
      "source": [
        "!pip install scrapy"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scrapy in /usr/local/lib/python3.6/dist-packages (1.7.3)\n",
            "Requirement already satisfied: Twisted>=13.1.0; python_version != \"3.4\" in /usr/local/lib/python3.6/dist-packages (from scrapy) (19.7.0)\n",
            "Requirement already satisfied: w3lib>=1.17.0 in /usr/local/lib/python3.6/dist-packages (from scrapy) (1.21.0)\n",
            "Requirement already satisfied: parsel>=1.5 in /usr/local/lib/python3.6/dist-packages (from scrapy) (1.5.2)\n",
            "Requirement already satisfied: cssselect>=0.9 in /usr/local/lib/python3.6/dist-packages (from scrapy) (1.1.0)\n",
            "Requirement already satisfied: lxml; python_version != \"3.4\" in /usr/local/lib/python3.6/dist-packages (from scrapy) (4.2.6)\n",
            "Requirement already satisfied: service-identity in /usr/local/lib/python3.6/dist-packages (from scrapy) (18.1.0)\n",
            "Requirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.6/dist-packages (from scrapy) (1.12.0)\n",
            "Requirement already satisfied: PyDispatcher>=2.0.5 in /usr/local/lib/python3.6/dist-packages (from scrapy) (2.0.5)\n",
            "Requirement already satisfied: queuelib in /usr/local/lib/python3.6/dist-packages (from scrapy) (1.5.0)\n",
            "Requirement already satisfied: pyOpenSSL in /usr/local/lib/python3.6/dist-packages (from scrapy) (19.0.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from Twisted>=13.1.0; python_version != \"3.4\"->scrapy) (19.1.0)\n",
            "Requirement already satisfied: hyperlink>=17.1.1 in /usr/local/lib/python3.6/dist-packages (from Twisted>=13.1.0; python_version != \"3.4\"->scrapy) (19.0.0)\n",
            "Requirement already satisfied: zope.interface>=4.4.2 in /usr/local/lib/python3.6/dist-packages (from Twisted>=13.1.0; python_version != \"3.4\"->scrapy) (4.6.0)\n",
            "Requirement already satisfied: PyHamcrest>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Twisted>=13.1.0; python_version != \"3.4\"->scrapy) (1.9.0)\n",
            "Requirement already satisfied: Automat>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from Twisted>=13.1.0; python_version != \"3.4\"->scrapy) (0.7.0)\n",
            "Requirement already satisfied: incremental>=16.10.1 in /usr/local/lib/python3.6/dist-packages (from Twisted>=13.1.0; python_version != \"3.4\"->scrapy) (17.5.0)\n",
            "Requirement already satisfied: constantly>=15.1 in /usr/local/lib/python3.6/dist-packages (from Twisted>=13.1.0; python_version != \"3.4\"->scrapy) (15.1.0)\n",
            "Requirement already satisfied: cryptography in /usr/local/lib/python3.6/dist-packages (from service-identity->scrapy) (2.7)\n",
            "Requirement already satisfied: pyasn1-modules in /usr/local/lib/python3.6/dist-packages (from service-identity->scrapy) (0.2.6)\n",
            "Requirement already satisfied: pyasn1 in /usr/local/lib/python3.6/dist-packages (from service-identity->scrapy) (0.4.7)\n",
            "Requirement already satisfied: idna>=2.5 in /usr/local/lib/python3.6/dist-packages (from hyperlink>=17.1.1->Twisted>=13.1.0; python_version != \"3.4\"->scrapy) (2.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from zope.interface>=4.4.2->Twisted>=13.1.0; python_version != \"3.4\"->scrapy) (41.2.0)\n",
            "Requirement already satisfied: asn1crypto>=0.21.0 in /usr/local/lib/python3.6/dist-packages (from cryptography->service-identity->scrapy) (0.24.0)\n",
            "Requirement already satisfied: cffi!=1.11.3,>=1.8 in /usr/local/lib/python3.6/dist-packages (from cryptography->service-identity->scrapy) (1.12.3)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi!=1.11.3,>=1.8->cryptography->service-identity->scrapy) (2.19)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-dq0KEjm5I4",
        "colab_type": "code",
        "outputId": "7cb47f92-ee06-4a07-f43f-7017271e7793",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuKDX55fm6MY",
        "colab_type": "text"
      },
      "source": [
        "## Linkear Google Drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ljMmopXYlLr",
        "colab_type": "text"
      },
      "source": [
        "## Montar Spider Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZXoTYPtHO6a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import scrapy\n",
        "import json\n",
        "\n",
        "class BlogSpider(scrapy.Spider):\n",
        "    name = 'blogspider'\n",
        "    # Podeis cambiar la url inicial por otra u otras paginas\n",
        "    start_urls = ['https://www.secretplaces.es/espana/hoteles']\n",
        "\n",
        "    def parse(self, response):\n",
        "        # Aqui scrapeamos los datos y los imprimimos a un fichero\n",
        "        for hotel in response.css('div.hotel-list-description-wrapper'):\n",
        "            nombre_hotel = hotel.css('h3 ::text').extract_first().strip()\n",
        "            tipo_alojamiento = hotel.css('div.hotel-list-type ::text').extract_first().replace(\"\\n\", \" \").replace(\",\", \"\").strip()\n",
        "            precio = hotel.css('ul.list-hotel-list-info li:nth-child(5) ::text').extract_first()\n",
        "              \n",
        "            # Print a un fichero\n",
        "            print(f\"{nombre_hotel}, {tipo_alojamiento}, {precio}\", file=filep)\n",
        "\n",
        "        # no hay paginacion, esto no hace falta\n",
        "        # for next_page in response.css('a.next-posts-link'):\n",
        "        #     yield response.follow(next_page, self.parse)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hq4352tpYyta",
        "colab_type": "text"
      },
      "source": [
        "## Ejecutar el Crawler + Scraper"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IriV_cmzYu3r",
        "colab_type": "code",
        "outputId": "ca6cb1b4-cf3d-4901-b4a6-0f362e5883b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 991
        }
      },
      "source": [
        "# Podeis cambiar la extension y el nombre del fichero data.txt\n",
        "filep = open('/content/drive/My Drive/webcrawling/data.csv', 'w')\n",
        "\n",
        "from scrapy.crawler import CrawlerProcess\n",
        "\n",
        "process = CrawlerProcess({\n",
        "    'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'\n",
        "})\n",
        "\n",
        "process.crawl(BlogSpider)\n",
        "process.start()\n",
        "filep.close()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-09-14 21:06:17 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: scrapybot)\n",
            "2019-09-14 21:06:17 [scrapy.utils.log] INFO: Versions: lxml 4.2.6.0, libxml2 2.9.8, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.6.8 (default, Jan 14 2019, 11:02:34) - [GCC 8.0.1 20180414 (experimental) [trunk revision 259383]], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Linux-4.14.137+-x86_64-with-Ubuntu-18.04-bionic\n",
            "2019-09-14 21:06:17 [scrapy.crawler] INFO: Overridden settings: {'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}\n",
            "2019-09-14 21:06:17 [scrapy.extensions.telnet] INFO: Telnet Password: da3269707cf91a49\n",
            "2019-09-14 21:06:17 [scrapy.middleware] INFO: Enabled extensions:\n",
            "['scrapy.extensions.corestats.CoreStats',\n",
            " 'scrapy.extensions.telnet.TelnetConsole',\n",
            " 'scrapy.extensions.memusage.MemoryUsage',\n",
            " 'scrapy.extensions.logstats.LogStats']\n",
            "2019-09-14 21:06:17 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
            "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
            " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
            " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
            " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
            " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
            " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
            " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
            "2019-09-14 21:06:17 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
            "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
            " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
            " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
            " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
            " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
            "2019-09-14 21:06:17 [scrapy.middleware] INFO: Enabled item pipelines:\n",
            "[]\n",
            "2019-09-14 21:06:17 [scrapy.core.engine] INFO: Spider opened\n",
            "2019-09-14 21:06:17 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
            "2019-09-14 21:06:17 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
            "2019-09-14 21:06:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.secretplaces.es/espana/hoteles> (referer: None)\n",
            "2019-09-14 21:06:19 [scrapy.core.engine] INFO: Closing spider (finished)\n",
            "2019-09-14 21:06:19 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
            "{'downloader/request_bytes': 248,\n",
            " 'downloader/request_count': 1,\n",
            " 'downloader/request_method_count/GET': 1,\n",
            " 'downloader/response_bytes': 105024,\n",
            " 'downloader/response_count': 1,\n",
            " 'downloader/response_status_count/200': 1,\n",
            " 'elapsed_time_seconds': 1.476322,\n",
            " 'finish_reason': 'finished',\n",
            " 'finish_time': datetime.datetime(2019, 9, 14, 21, 6, 19, 363976),\n",
            " 'log_count/DEBUG': 1,\n",
            " 'log_count/INFO': 10,\n",
            " 'memusage/max': 176009216,\n",
            " 'memusage/startup': 176009216,\n",
            " 'response_received_count': 1,\n",
            " 'scheduler/dequeued': 1,\n",
            " 'scheduler/dequeued/memory': 1,\n",
            " 'scheduler/enqueued': 1,\n",
            " 'scheduler/enqueued/memory': 1,\n",
            " 'start_time': datetime.datetime(2019, 9, 14, 21, 6, 17, 887654)}\n",
            "2019-09-14 21:06:19 [scrapy.core.engine] INFO: Spider closed (finished)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ap-54yCYp3j",
        "colab_type": "text"
      },
      "source": [
        "## Descargar resultados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqyCtD32KMFY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "files.download('/content/drive/My Drive/webcrawling/data.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEWA_LAwiAer",
        "colab_type": "text"
      },
      "source": [
        "## Enviar resultados a Google Cloud Storage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPSCbWuhiFwl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Necesitas saber el id de tu proyecto en la consola cloud de google (esta arriba a la izquierda)\n",
        "project_id = 'heroic-alpha-252318'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_UYtwtFhgfY",
        "colab_type": "code",
        "outputId": "92bf1c7b-2d35-460f-af41-de4d8cc38e54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "# Importa las librerias necesarias y autenticas con google cloud\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-09-14 21:07:04 [google.auth.transport.requests] DEBUG: Making request: POST https://accounts.google.com/o/oauth2/token\n",
            "2019-09-14 21:07:04 [urllib3.connectionpool] DEBUG: Starting new HTTPS connection (1): accounts.google.com:443\n",
            "2019-09-14 21:07:04 [urllib3.connectionpool] DEBUG: https://accounts.google.com:443 \"POST /o/oauth2/token HTTP/1.1\" 200 None\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rE-t3f2Th1n_",
        "colab_type": "code",
        "outputId": "29c77345-e14a-4014-98bc-ae575f1e72af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 557
        }
      },
      "source": [
        "# Ahora nos traemos lo necesario para cloud storage\n",
        "from googleapiclient.discovery import build\n",
        "gcs_service = build('storage', 'v1')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-09-14 21:07:10 [googleapiclient.discovery_cache] WARNING: file_cache is unavailable when using oauth2client >= 4.0.0 or google-auth\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/googleapiclient/discovery_cache/__init__.py\", line 36, in autodetect\n",
            "    from google.appengine.api import memcache\n",
            "ModuleNotFoundError: No module named 'google.appengine'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/googleapiclient/discovery_cache/file_cache.py\", line 33, in <module>\n",
            "    from oauth2client.contrib.locked_file import LockedFile\n",
            "ModuleNotFoundError: No module named 'oauth2client.contrib.locked_file'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/googleapiclient/discovery_cache/file_cache.py\", line 37, in <module>\n",
            "    from oauth2client.locked_file import LockedFile\n",
            "ModuleNotFoundError: No module named 'oauth2client.locked_file'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/googleapiclient/discovery_cache/__init__.py\", line 41, in autodetect\n",
            "    from . import file_cache\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/googleapiclient/discovery_cache/file_cache.py\", line 41, in <module>\n",
            "    'file_cache is unavailable when using oauth2client >= 4.0.0 or google-auth')\n",
            "ImportError: file_cache is unavailable when using oauth2client >= 4.0.0 or google-auth\n",
            "2019-09-14 21:07:10 [googleapiclient.discovery] INFO: URL being requested: GET https://www.googleapis.com/discovery/v1/apis/storage/v1/rest\n",
            "2019-09-14 21:07:10 [google.auth._default] WARNING: No project ID could be determined. Consider running `gcloud config set project` or setting the GOOGLE_CLOUD_PROJECT environment variable\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZ0dSIShiJ8U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Ahora necesitas el nombre de tu segmento, para ello crea o busca uno aqui https://console.cloud.google.com/storage/browser\n",
        "bucket_name = 'dataproc-78c05970-c291-47f3-843a-d313935d5526-europe-west1'\n",
        "\n",
        "# Tambien puedes definir el nombre del fichero\n",
        "file_name = 'hoteles.csv'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9IFyxX3jijcz",
        "colab_type": "code",
        "outputId": "adaefb3e-f6af-4956-f437-a9a43da547f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "# Ahora cogemos el fichero del resultado del scrapping y lo subimos al bucket\n",
        "from googleapiclient.http import MediaFileUpload\n",
        "\n",
        "media = MediaFileUpload('/content/drive/My Drive/webcrawling/data.csv', \n",
        "                        mimetype='text/plain',\n",
        "                        resumable=True)\n",
        "\n",
        "request = gcs_service.objects().insert(bucket=bucket_name, \n",
        "                                       name=file_name,\n",
        "                                       media_body=media)\n",
        "\n",
        "response = None\n",
        "while response is None:\n",
        "  # _ is a placeholder for a progress object that we ignore.\n",
        "  # (Our file is small, so we skip reporting progress.)\n",
        "  _, response = request.next_chunk()\n",
        "\n",
        "print('Upload completo')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-09-14 21:07:27 [googleapiclient.discovery] INFO: URL being requested: POST https://www.googleapis.com/upload/storage/v1/b/dataproc-78c05970-c291-47f3-843a-d313935d5526-europe-west1/o?name=hoteles.csv&alt=json&uploadType=resumable\n",
            "2019-09-14 21:07:27 [google_auth_httplib2] DEBUG: Making request: POST https://accounts.google.com/o/oauth2/token\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Upload completo\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zCL1SQdlGfC",
        "colab_type": "text"
      },
      "source": [
        "### Cuando la subida haya finalizado, los datos aparecerán en el explorador de datos almacenados de la consola de Cloud de tu proyecto:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "colab_type": "code",
        "id": "Lvz1BspfpvTl",
        "outputId": "489966dd-16e9-4e85-e710-2cef7304b4a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print('https://console.cloud.google.com/storage/browser?project=' + project_id)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "https://console.cloud.google.com/storage/browser?project=heroic-alpha-252318\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}